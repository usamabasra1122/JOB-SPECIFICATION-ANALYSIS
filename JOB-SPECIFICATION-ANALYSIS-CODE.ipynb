{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Final-NLP Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_SuZOh-4EX9",
        "outputId": "fa74b796-3d36-463a-8d81-832eb161cb36"
      },
      "source": [
        "pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-rBsUlm4HKz",
        "outputId": "e87e45d7-6681-4529-c830-1f8cb28b341d"
      },
      "source": [
        "pip install python-docx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=c4fa27d99a85fc5f83da7d3132ab3d8b7fa4fed9ff8a9800be2f37f14869e940\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gJHxdrc4Oni"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsiMAE136xM",
        "outputId": "c47b87b3-fc0e-4b77-b5f6-c4f1801d9c1c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54bLCYU43ybE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e9e567-cc3d-49c4-ffc4-b06ab77da490"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.util import filter_spans\n",
        "from spacy.tokens import Span\n",
        "from spacy.matcher import Matcher\n",
        "from spacy import displacy\n",
        "\n",
        "import docx\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "from IPython.display import HTML, display\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "from nltk import tokenize\n",
        "from operator import itemgetter\n",
        "import math\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import difflib\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "exclude_item_list=['China', 'London', 'US', 'Us', 'United Kingdom', 'Singapore', 'year', 'years', 'work', 'preffered']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ajy4KBe3ybL"
      },
      "source": [
        "def main(filename):\n",
        "\n",
        "    \n",
        "\n",
        "    document = docx.Document(filename)\n",
        "    docText = '\\n\\n'.join(\n",
        "        paragraph.text for paragraph in document.paragraphs\n",
        "    )\n",
        "    txt = docText\n",
        "        # #  Soft Skill\n",
        "    lst = soft_skill_patterns(txt)\n",
        "    lst = list(set(lst))\n",
        "\n",
        "    #opeing the soft skills file\n",
        "    f = open('soft_skills.txt', 'r+',encoding=\"utf-8\")\n",
        "    skills_list = f.read().splitlines()\n",
        "\n",
        "\n",
        "    temp_list=[]\n",
        "    for item in lst:\n",
        "        #print(item)\n",
        "        chk=difflib.get_close_matches(item, skills_list, n=1, cutoff=0.7)\n",
        "        if len(chk)>0 and item not in temp_list:\n",
        "            temp_list.append(item)\n",
        "\n",
        "            \n",
        "    soft_skill=validate_list_by_tfidf(txt,temp_list)\n",
        "    print(\"--------Soft Skills-----------\")\n",
        "    print(set(soft_skill))\n",
        "    \n",
        "    \n",
        "    # Hard Skill\n",
        "\n",
        "    hard_lst=hard_skill_patterns(txt)\n",
        "\n",
        "    # Extracting Experiences\n",
        "    experience = extract_experience(txt)\n",
        "\n",
        "    for item in experience:\n",
        "        if item not in hard_lst:\n",
        "            hard_lst.append(item)\n",
        "\n",
        "    hard_lst=set(hard_lst)\n",
        "    mylist=sorted(list(hard_lst), key=len, reverse=True)\n",
        "    temp_list=mylist\n",
        "\n",
        "    \n",
        "    remove_item=[]\n",
        "    for i in range(len(mylist)):\n",
        "        for j in range(len(mylist)):\n",
        "            if i!=j:\n",
        "                \n",
        "                if mylist[i] in mylist[j]:\n",
        "                    #print(mylist[i], '   ', mylist[j])\n",
        "                    #print(fuzz.ratio(mylist[i], mylist[j]))\n",
        "                    if fuzz.ratio(mylist[i], mylist[j]) > 70:\n",
        "                        if mylist[i] not in remove_item:\n",
        "                            remove_item.append(mylist[i])\n",
        "\n",
        "    for i in remove_item:\n",
        "        if i in temp_list:\n",
        "            temp_list.remove(i)\n",
        "\n",
        "    #print(temp_list)\n",
        "    \n",
        "    \n",
        "    \n",
        "    f = open('hard_skills.txt', 'r+')\n",
        "    skills_list = f.read().splitlines()\n",
        "    skills_list=list(set(skills_list))\n",
        "    \n",
        "    t1=list(set(temp_list))\n",
        "\n",
        "    chk=[]\n",
        "    for i in range(len(skills_list)):\n",
        "        for j in range(len(t1)):   \n",
        "            #print(skills_list[i])\n",
        "\n",
        "            if skills_list[i].lower() in t1[j].lower():\n",
        "                #print(skills_list[i], '   ', t1[j])\n",
        "                #print(fuzz.ratio(skills_list[i].lower(), t1[j].lower()))\n",
        "                if fuzz.ratio(skills_list[i].lower(), t1[j].lower()) > 70:\n",
        "                    #print(skills_list[i], '   ', t1[j])\n",
        "                    #print(fuzz.ratio(skills_list[i].lower(), t1[j].lower()))\n",
        "                    if t1[j] not in chk:\n",
        "                        chk.append(t1[j])\n",
        "    \n",
        "    updated=chk.copy()\n",
        "    #print(chk)\n",
        "    for i in range(len(chk)):\n",
        "        for j in range(i,len(chk)):\n",
        "            if i!=j:\n",
        "                #print(chk[i], '   ', chk[j])\n",
        "                #print(fuzz.ratio(chk[i], chk[j]))\n",
        "                    \n",
        "                if fuzz.ratio(chk[i], chk[j]) > 70:\n",
        "                    #print('test')\n",
        "                    #print(chk[i], '   ', chk[j])\n",
        "                    #print(fuzz.ratio(chk[i], chk[j]))\n",
        "                    item = chk[j]\n",
        "                    if chk[j] in updated:\n",
        "                        #print(item)\n",
        "                        #print(len(chk))\n",
        "                        updated.remove(item)\n",
        "                        #print(len(chk))\n",
        "                        #print(len(updated))\n",
        "        \n",
        "    \n",
        "    #print(updated)\n",
        "    \n",
        "    hard_skill_list=updated\n",
        "    ''' \n",
        "    hard_skill_list=list(map(lambda x: x.replace('',''),hard_skill_list))\n",
        "    for item in soft_skill:\n",
        "        if item.lower() in hard_skill_list:\n",
        "            print(item)\n",
        "    \n",
        "    '''   \n",
        "    print(\"--------Hard Skills-----------\")\n",
        "    print(set(hard_skill_list))\n",
        "    \n",
        "    \n",
        "    data={\n",
        "        \"Soft_skils\":soft_skill,\n",
        "        \"Hard_skills\":hard_skill_list\n",
        "    }\n",
        "    \n",
        "    temp=json.dumps(data, indent = 4) \n",
        "    temp=json.loads(temp)\n",
        "    all_skills=json.dumps(temp, indent=4)\n",
        "    #print(json.dumps(temp, indent=4))\n",
        "    return all_skills"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4_gch293ybN"
      },
      "source": [
        "def clean_text(raw):\n",
        "    try:\n",
        "        # remove carriage returns and new lines\n",
        "        raw = raw.replace('\\r', '')\n",
        "        raw = raw.replace('\\n', ' ')\n",
        "        \n",
        "        # brackets appear in all instances\n",
        "        raw = raw.replace('[', '')\n",
        "        raw = raw.replace(']', '')\n",
        "        raw = raw.replace(')', '')\n",
        "        raw = raw.replace('(', '')\n",
        "        \n",
        "        # removing html tags\n",
        "        clean_html = re.compile('<.*?>')\n",
        "        clean_text = re.sub(clean_html, ' ', raw)\n",
        "        \n",
        "        # removing duplicate whitespace in between words\n",
        "        clean_text = re.sub(\" +\", \" \", clean_text) \n",
        "        \n",
        "        # stripping first and last white space \n",
        "        clean_text = clean_text.strip()\n",
        "        \n",
        "        # commas had multiple spaces before and after in each instance\n",
        "        clean_text = re.sub(\" , \", \", \", clean_text) \n",
        "        \n",
        "        # eliminating the extra comma after a period\n",
        "        clean_text = clean_text.replace('.,', '.')        \n",
        "        \n",
        "        clean_text = re.sub('-', '', clean_text)\n",
        "        clean_text = re.sub('_', '', clean_text)\n",
        "        clean_text = re.sub('\\[[^]]*\\]', '', clean_text)\n",
        "        #clean_text = re.sub(r'\\n',' ',clean_text)\n",
        "        clean_text = re.sub(r'[^\\w\\s]',' ',clean_text)\n",
        "        \n",
        "        clean_text = re.sub(r'[0-9]+','',clean_text)\n",
        "        clean_text = clean_text.replace(u'\\xa0', u' ')\n",
        "        \n",
        "    except:\n",
        "        clean_text = np.nan\n",
        "        \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def extract_experience(text):\n",
        "    experience_word_list=[]\n",
        "    \n",
        "    test=re.findall(r'(experience[^.]*)', text,flags=re.IGNORECASE)\n",
        "    test = ' '.join(test)\n",
        "    test=nlp(test)\n",
        "\n",
        "\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun\", [[{\"POS\": \"NOUN\"},{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(test)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = test[matches[i][1]:matches[i][2]]\n",
        "        #collect_all_matched_skill.append(str(token))\n",
        "        if str(token).lower() not in exclude_item_list:\n",
        "            #print(token)\n",
        "            experience_word_list.append(str(token))\n",
        "\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP & Noun\", [[{\"POS\": \"PROPN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(test)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = test[matches[i][1]:matches[i][2]]\n",
        "        #collect_all_matched_skill.append(str(token))\n",
        "        if str(token).lower() not in exclude_item_list:\n",
        "            #print(token)\n",
        "            experience_word_list.append(str(token))\n",
        "        \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"ADJ & Noun\", [[{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(test)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = test[matches[i][1]:matches[i][2]]\n",
        "        #collect_all_matched_skill.append(str(token))\n",
        "        if str(token).lower() not in exclude_item_list:\n",
        "            #print(token)\n",
        "            experience_word_list.append(str(token).lower())\n",
        "    \n",
        "    return experience_word_list\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text_tokens = text.split(\" \")\n",
        "    tokens_filtered= [word for word in text_tokens if not word.lower() in stop_words]\n",
        "    return (\" \").join(tokens_filtered)\n",
        "\n",
        "\n",
        "def check_sent(word, sentences): \n",
        "    final = [all([w in x for w in word]) for x in sentences] \n",
        "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
        "    return int(len(sent_len))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SOK-nn_3ybO"
      },
      "source": [
        "def calculate_tfidf(text):\n",
        "    doc = text.replace('\\r', '')\n",
        "    doc = doc.replace('\\n', ' ')\n",
        "\n",
        "    # brackets appear in all instances\n",
        "    doc = doc.replace('[', '')\n",
        "    doc = doc.replace(']', '')\n",
        "    doc = doc.replace(')', '')\n",
        "    doc = doc.replace('(', '')\n",
        "    doc = doc.replace(',', '')\n",
        "    doc = doc.replace('—', '')\n",
        "    doc = doc.replace('-', '')\n",
        "\n",
        "        \n",
        "    total_words = doc.split()\n",
        "    total_word_length = len(total_words)\n",
        "    #print(total_word_length)\n",
        "    total_sentences = tokenize.sent_tokenize(doc)\n",
        "    total_sent_len = len(total_sentences)\n",
        "    #print(total_sent_len)\n",
        "    tf_score = {}\n",
        "    for each_word in total_words:\n",
        "        each_word = each_word.replace('.','')\n",
        "        if each_word not in stop_words:\n",
        "            if each_word in tf_score:\n",
        "                tf_score[each_word] += 1\n",
        "            else:\n",
        "                tf_score[each_word] = 1\n",
        "\n",
        "    # Dividing by total_word_length for each dictionary element\n",
        "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
        "    #print(tf_score)\n",
        "    \n",
        "    idf_score = {}\n",
        "    for each_word in total_words:\n",
        "        each_word = each_word.replace('.','')\n",
        "        if each_word not in stop_words:\n",
        "            if each_word in idf_score:\n",
        "                idf_score[each_word] = check_sent(each_word, total_sentences)\n",
        "            else:\n",
        "                idf_score[each_word] = 1\n",
        "\n",
        "    # Performing a log and divide\n",
        "    \n",
        "\n",
        "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
        "    #print(idf_score)\n",
        "    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}\n",
        "    #print(tf_idf_score)\n",
        "    return tf_idf_score\n",
        "\n",
        "\n",
        "def validate_list_by_tfidf(txt, lst):\n",
        "    prob_dic = calculate_tfidf(txt)\n",
        "    cleaned_list=[]\n",
        "    test=lst\n",
        "    test=list(set(test))\n",
        "\n",
        "    updated_dic = dict((k.lower(), v) for k,v in prob_dic.items())\n",
        "\n",
        "    chk=[k.lower()  for  k in  updated_dic.keys()]\n",
        "    temp_dict={}\n",
        "    total=0\n",
        "    for words in test:\n",
        "        #print(words)\n",
        "        i = words.split(' ')\n",
        "\n",
        "        sum=0\n",
        "\n",
        "        for j in i:\n",
        "            if j in chk:\n",
        "\n",
        "                #print(j, updated_dic[j])\n",
        "                sum+=updated_dic[j]\n",
        "        #print(sum)\n",
        "        temp_dict[words]=sum\n",
        "        total+=sum\n",
        "        #print()\n",
        "\n",
        "    #print(total/len(test))\n",
        "    limit = round(total/len(test), 4)\n",
        "    #print(limit)\n",
        "    \n",
        "    for i,j in temp_dict.items():\n",
        "        if j > limit:\n",
        "            #print(i)\n",
        "            cleaned_list.append(str(i))\n",
        "        #else:\n",
        "            #print('-------  ', i,j)\n",
        "        \n",
        "    return cleaned_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLcKxv8C3ybP"
      },
      "source": [
        "def hard_skill_patterns(text):\n",
        "    collect_all_matched_skill=[]\n",
        "    words_pos_dict={}\n",
        "    doc = clean_text(text)\n",
        "    doc = remove_stopwords(doc)\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    \n",
        "    for token in doc:\n",
        "        #if not token.is_punct and not token.is_stop  and token.lower_ not in stop_words and token.lemma_ not in stop_words and token.pos_ != 'SPACE':\n",
        "        #print(token, ' -> ', token.pos_, ' -> ', )\n",
        "        if str(token) not in words_pos_dict:\n",
        "            item = str(token)\n",
        "            words_pos_dict[item]=token.pos_\n",
        "            \n",
        "    #pattern \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun\", [[{\"POS\": \"NOUN\"},{\"POS\": \"NOUN\",\"OP\": \"+\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())\n",
        "\n",
        "    #pattern \n",
        "    \n",
        "    '''\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP\", [[{\"POS\": \"PROPN\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())\n",
        "    '''       \n",
        "    #pattern \n",
        "    \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP\", [[{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\",\"OP\": \"+\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())\n",
        "    \n",
        "            \n",
        "    #pattern\n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP & Noun\", [[{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())    \n",
        "    \n",
        "    #pattern \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP & Verb\", [[{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}, {\"POS\": \"VERB\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())    \n",
        "            \n",
        "    #pattern \n",
        "    \n",
        "   \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Verb & Noun\", [[{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower()) \n",
        "    \n",
        "    \n",
        "    #pattern \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROP\", [[{\"POS\": \"PROPN\"},{\"POS\": \"NOUN\",\"OP\": \"+\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            #print(token)\n",
        "            collect_all_matched_skill.append(str(token).lower())\n",
        "            \n",
        "    return collect_all_matched_skill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ3WYyUJ3ybQ"
      },
      "source": [
        "def soft_skill_patterns(text):\n",
        "    collect_all_matched_skill=[]\n",
        "\n",
        "    doc = clean_text(text)\n",
        "    doc = remove_stopwords(doc)\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    #pattern 1 \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun\", [[{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun\", [[{\"POS\": \"PROPN\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "     \n",
        "    \n",
        "    #pattern 2\n",
        "    \n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"ADJ\", [[{\"POS\": \"ADJ\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "    \n",
        "    #pattern 3\n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Verb & PROP & Noun\", [[{\"POS\": \"VERB\"},{\"POS\": \"PROPN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "            \n",
        "    #pattern 4\n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Verb\", [[{\"POS\": \"VERB\"}, {\"POS\": \"VERB\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower())     \n",
        "    \n",
        "    #pattern 5\n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Verb & Noun\", [[{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower())     \n",
        "            \n",
        "    #pattern 6\n",
        "    \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"ADJ & ADJ\", [[{\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "    \n",
        "    #pattern 7 \n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun & Verb\", [[{\"POS\": \"NOUN\"},{\"POS\": \"VERB\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "            \n",
        "            \n",
        "    #pattern 8\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Adj & Noun\", [[{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "            \n",
        "    #pattern 9\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Adj & Verb\", [[{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "            \n",
        "            \n",
        "    #pattern 11\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Adj & Noun\", [[{\"POS\": \"VERB\"},{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "                \n",
        "    #pattern 12\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun & Verb\", [[{\"POS\": \"NOUN\"},{\"POS\": \"NOUN\"},{\"POS\": \"VERB\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "                \n",
        "                \n",
        "    #pattern 13\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Verb & Noun\", [[{\"POS\": \"VERB\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "                \n",
        "    #pattern 14\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Adj & Noun\", [[{\"POS\": \"ADJ\"}, {\"POS\": \"VERB\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower())  \n",
        "                \n",
        "                \n",
        "    #pattern 15\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Adj & Noun\", [[{\"POS\": \"ADJ\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower())  \n",
        "            \n",
        "            \n",
        "    #pattern 16\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"PROPN & Noun\", [[{\"POS\": \"PROPN\"},{\"POS\": \"NOUN\"}]])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "    \n",
        "    \n",
        "    for i in range(0,len(matches)):\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        \n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower())  \n",
        "        \n",
        "    #print()        \n",
        "    #pattern 17\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"Noun\", [[{\"POS\": \"NOUN\"},{\"POS\": \"VERB\"}]])\n",
        "\n",
        "    #doc = nlp(u'what are the main issues')\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        token = doc[matches[i][1]:matches[i][2]]\n",
        "        # append token to list\n",
        "        if str(token) not in collect_all_matched_skill:\n",
        "            if not any(word in str(token) for word in exclude_item_list):\n",
        "\n",
        "                #print(token)\n",
        "                #collect_all_matched_skill.append(str(token))\n",
        "                collect_all_matched_skill.append(str(token).lower()) \n",
        "    \n",
        "    return collect_all_matched_skill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0wYMUm63ybT",
        "outputId": "70116454-29ad-4320-ca77-fba59f4f2fa4"
      },
      "source": [
        "f1=\"Job 1, Risk Management Financial Services.docx\"\n",
        "f2=\"Job 2, Sports Phsiotherapist.docx\"\n",
        "f3=\"Job 3, Retail Branch Manager.docx\"\n",
        "f4=\"Job 4, Secondary School Teacher.docx\"\n",
        "f5=\"Job 5, Senior Marketing Manager.docx\"\n",
        "\n",
        "skills=main(f5)\n",
        "print(skills)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------Soft Skills-----------\n",
            "{'active users', 'cultures team player', 'relative key', 'sexual orientation', 'marketing', 'influencing skills', 'required willingness travel', 'ability launch', 'proven ability', 'high attention detail', 'making decisions', 'team player', 'complex decisions', 'key accountabilities', 'negotiation influencing skills', 'eager learn', 'willing partner'}\n",
            "--------Hard Skills-----------\n",
            "{'dress schedule', 'research papers', 'market rate salary', 'leaders stakeholders', 'identity framework', 'job types', 'competitor intelligence', 'china singapore', 'empowers sales channels', 'gender identity', 'team people', 'strategies product launches', 'team player', 'core archetypes', 'business acumen', 'cov guidelines', 'stakeholder relationships', 'authority marketing effectiveness', 'developing product marketing', 'case studies', 'united kingdom'}\n",
            "{\n",
            "    \"Soft_skils\": [\n",
            "        \"active users\",\n",
            "        \"negotiation influencing skills\",\n",
            "        \"cultures team player\",\n",
            "        \"sexual orientation\",\n",
            "        \"marketing\",\n",
            "        \"influencing skills\",\n",
            "        \"making decisions\",\n",
            "        \"willing partner\",\n",
            "        \"relative key\",\n",
            "        \"high attention detail\",\n",
            "        \"team player\",\n",
            "        \"complex decisions\",\n",
            "        \"eager learn\",\n",
            "        \"required willingness travel\",\n",
            "        \"ability launch\",\n",
            "        \"proven ability\",\n",
            "        \"key accountabilities\"\n",
            "    ],\n",
            "    \"Hard_skills\": [\n",
            "        \"strategies product launches\",\n",
            "        \"dress schedule\",\n",
            "        \"gender identity\",\n",
            "        \"identity framework\",\n",
            "        \"team people\",\n",
            "        \"competitor intelligence\",\n",
            "        \"authority marketing effectiveness\",\n",
            "        \"core archetypes\",\n",
            "        \"job types\",\n",
            "        \"leaders stakeholders\",\n",
            "        \"developing product marketing\",\n",
            "        \"case studies\",\n",
            "        \"empowers sales channels\",\n",
            "        \"united kingdom\",\n",
            "        \"stakeholder relationships\",\n",
            "        \"team player\",\n",
            "        \"market rate salary\",\n",
            "        \"business acumen\",\n",
            "        \"china singapore\",\n",
            "        \"cov guidelines\",\n",
            "        \"research papers\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfW7DjBY3ybU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}